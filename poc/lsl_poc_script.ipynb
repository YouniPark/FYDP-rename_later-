{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7267ba2-5378-40be-a977-5230c873fe0d",
   "metadata": {},
   "source": [
    "# Detect Face Boundaries in Camera Footage\n",
    "\n",
    "Written by Yoonhee Park\n",
    "\n",
    "github: YouniPark\n",
    "\n",
    "Thur, Sep 25, 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae94fb0-d2e2-4169-b230-3f089bb27dd4",
   "metadata": {},
   "source": [
    "## LSL Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcf9356-4a19-4dba-b9d8-d5a2174447e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylsl import StreamInfo, StreamOutlet, local_clock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532f2580-39f2-45b5-8701-481c93250fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Independent Streams (EEG, eye tracker, markers)\n",
    "# LSL will automatically synchronizes data to the same global clock\n",
    "\n",
    "info_eeg = StreamInfo()\n",
    "outlet_eeg = StreamOutlet(info_eeg)\n",
    "\n",
    "# Eye tracker gaze coordinate stream (# channels: x, y, z?) 2D or 3D?\n",
    "# nominal_srate: sampling rate in Hz\n",
    "info_gaze = StreamInfo(name = 'EyeTracker', type = 'EyeGaze', channel_count = 2,\n",
    "                       nominal_srate = 60, channel_format, source_id)\n",
    "outlet_gaze = StreamOutlet(info_gaze)\n",
    "\n",
    "# Event marker stream (1 channel, string)\n",
    "info_markers = StreamInfo('Markers', 'Events', 1, 0, 'string', 'marker')\n",
    "outlet_markers = StreamOutlet(info_markers) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee09ef-7a3f-4cfc-8c10-8f16a2e82689",
   "metadata": {},
   "source": [
    "## Real-time Video Streaming from Tobii Camera as an Input (IP address of camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7911fec7-4b21-4a14-8f33-2d7a40bfc304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5d4a7e-15c5-41f6-8fdc-70e1f3734526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Face Detection Model\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaddb585-0da0-4563-885d-b2853c02846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Up Video Stream\n",
    "\n",
    "ip_url = \"\" # IP URL of Tobii camera\n",
    "cap = cv2.VideoCapture(ip_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a410d-2b63-4957-9525-3589ac510f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Eye Tracker Output\n",
    "\n",
    "# time, coordinate (x, y) \n",
    "time = 123\n",
    "x_gaze = 123\n",
    "y_gaze = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491174d-849a-45a7-9c02-c719c6186198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying Faces in the Video Stream\n",
    "\n",
    "# Function to detect faces in the video stream and draw a bounding box around them:\n",
    "def detect_bounding_box(vid):\n",
    "    \n",
    "    gray_image = cv2.cvtColor(vid, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray_image, 1.1, 5, minSize=(40, 40))\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(vid, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "        # Print coordinates\n",
    "        print(f\"Face detected at X:{x}, Y:{y}, Width:{w}, Height:{h}\")\n",
    "        \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fa45c8-b274-4ca2-a2b6-10ef64346269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Loop for Real-Time Face Detection\n",
    "\n",
    "event_time = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    result, video_frame = video_capture.read()  # read frames from the video\n",
    "    if result is False:\n",
    "        break  # terminate the loop if the frame is not read successfully\n",
    "\n",
    "    faces = detect_bounding_box(\n",
    "        video_frame\n",
    "    )  # apply the function we created to the video frame\n",
    "\n",
    "    # --- Algorithm to mark event (Vivien) \n",
    "    # --- #\n",
    "\n",
    "    cv2.imshow(\n",
    "        \"My Face Detection Project\", video_frame\n",
    "    )  # display the processed frame in a window named \"My Face Detection Project\"\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10ff0af-5f37-4460-9583-8ed8b00ada6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy bio signals generator to LSL -> get its time stamp -> mark it as start of fixation time\n",
    "# Set up a dummy event marker and test with LabRecorder to see if they receive event marker"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
